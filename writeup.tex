\documentclass{article}
\usepackage{graphicx} % Required for inserting images\
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmax}{argmax}

\title{Go By Vibes}
\author{Misha Lvovsky}
\date{August 2023}

\begin{document}

\maketitle

\section{Problem Statement}

We want to create a smooth embedding space which is analogous to our joint space.
Let's define that a neighborhood in our embedding space has a size of $1$ unit. This means that within $1$ unit all actions and states should be similar and should yield similar results. We can tie these vicinities together by saying that a certain vicinity should correspond with a certain amount of elapsed time. A certain action should be in the vicinity of another action if the resultant states they reach from a given source state are themselves in the same smaller vicinity.

\section{Spaces}

\begin{definition} [State Space]
Let $\mathcal{S}$ signify the space of all possible states for the system.
\end{definition}
\begin{definition} [Action Space]
    Let $\mathcal{A}$ signify the space of all possible actions the agent can make
\end{definition}
\begin{definition} [Embedding Space]
Let $\mathcal{Z}^s$ and $\mathcal{Z}^a$ signify the spaces of all possible embeddings of states and actions respectively.
\end{definition}


\section{Transformations}

\subsection*{Definitions}

\begin{definition} [Encoders]
    Let $\mathcal{E}$ represent the encoders which map $\mathcal{E}_s: \mathcal{S}\rightarrow\mathcal{Z}^s$ and $\mathcal{E}_a: \mathcal{A} \times \mathcal{Z}^s \rightarrow \mathcal{Z}^a$
\end{definition}
\begin{definition} [Decoders]
    Let $\mathcal{D}$ represent the decoders which map $\mathcal{D}_s: \mathcal{Z}^s \rightarrow \mathcal{S}$ and $\mathcal{D}_a: \mathcal{Z}^a \times \mathcal{Z}^s \rightarrow \mathcal{A}$.
\end{definition}
\begin{definition} [Forward Model]
    Let $\mathcal{F}$ represent the forward model which maps $\mathcal{F}: \mathcal{Z}^s \times \mathcal{Z}^a \rightarrow \mathcal{Z}^s$ where the inputs are state and action at time step $t$ and the output is the state at time step $t+1$
\end{definition}
\begin{definition} [Distance Metric]
    Let $d$ represent distance metrics in the latent state and action spaces $d_s: \mathcal{Z}^s \times \mathcal{Z}^s \rightarrow \mathbb{R}$ and $d_a: \mathcal{Z}^a \times \mathcal{Z}^a \rightarrow \mathbb{R}$
\end{definition}

\subsection*{Details}

\begin{itemize}
    \item The action encoder is conditioned on the state because the same actions might be functionally different in different contexts.
    \item When writing $\mathcal{E}$ and $\mathcal{D}$ the subscript will be omitted to simplify notation.
          % \item $\mathcal{FE}(s, a)$ is defined as $\mathcal{F} \left( \mathcal{E}(s), \mathcal{E}(s, a) \right)$ to simplify notation.
          % \item $z^s$ and $z^a$ are defined as $\mathcal{E}(s)$ and $\mathcal{E}(a, s)$ respectively to simplify notation.
\end{itemize}


\section{Representation Properties}

To create an abstract representation of states and actions we search for a fixed size set of actions which enable traversal over maximal set of states.
To realize this we apply a few constraints.

\subsection*{Smoothness Constraint}

The purpose of this create "neighborhoods" in the state and action spaces.
In these neighborhoods latent states and actions are grouped together based on a pre-defined distance metric.\\

\noindent
Formally for any state action pairs $(s_i, a_i)$ and $(s_j, a_j)$, the states and actions should be in the same neighborhood if the future states they lead to are also in the same neighborhood.
\begin{align*}
     & \forall (s_i, a_i) \in \mathcal{S} \times \mathcal{A},                                                      \\
     & \forall (a_i, a_j) \in \mathcal{S} \times \mathcal{A}:                                                      \\
     & \qquad \text{Let } z_{si} = \mathcal{E}(s_i) \text{ and } z_{sj} = \mathcal{E}(s_j)                         \\
     & \qquad \text{Let } z_{ai} = \mathcal{E}(a_i, z_{si}) \text{ and } z_{aj} = \mathcal{E}(a_j, z_{sj})         \\
     & \qquad \text{Let } z_{si}' = \mathcal{F}(z_{si}, z_{ai}) \text{ and } z_{sj}' = \mathcal{F}(z_{sj}, z_{aj}) \\
     & \qquad d(z_{si}, z_{sj}) < 1 \land d(z_{ai}, z_{aj}) < 1 \iff d(z_{si}', z_{sj}') < 1
\end{align*}



\subsection*{Radius Constraint}

The purpose of this constraint is to limit the number of neighborhoods which can exist.
The smoothness constraint alone does not force abstract representation because the mapping can simply make every neighborhood contain only one action.
To force the representation to learn an interesting abstraction we can modify two properties of the latent action space.
First we can limit the number of interfaces that one neighborhood may have with another.
In my experiments this variation takes the form of varying the dimensionality of the latent action representations.
For example, for action representations in $\mathbb{R}^2$ if the $l$-$1$ norm is used as the distance metric each action neighborhood can have 4 adjacent neighborhoods.
Second we can modify the radius of the action space.
This takes the form of limiting the maximal distance between two actions.
Between those two options we can tune the action space such that it produces the desired level of abstraction.\\

\noindent
Formally we say that for all state and action pairs $(s_i, a_i)$ and $(s_j, a_j)$ the distance metric between their latent representations should be less than twice the radius.

\begin{align*}
     & \forall(s_i, a_i) \in \mathcal{S} \times \mathcal{A}                                                \\
     & \forall(s_j, a_j) \in \mathcal{S} \times \mathcal{A}                                                \\
     & \qquad \text{Let } z_{si} = \mathcal{E}(s_i) \text{ and } z_{sj} = \mathcal{E}(s_j)                 \\
     & \qquad \text{Let } z_{ai} = \mathcal{E}(a_i, z_{si}) \text{ and } z_{aj} = \mathcal{E}(a_j, z_{sj})
\end{align*}

\subsection*{Dispersion Objective}

The purpose of this objective is to create a maximally abstract representation of our system states given the constraints on the action space.
By maximizing the number of state neighborhoods we force the representation to find the most abstracted form of our limited number of actions which best explain all of the possible state transitions.

\noindent
Formally we want our $\mathcal{E}$ to be the one that maximizes the distance between all state transitions while satisfying the constraints.

\begin{align*}
     & \mathcal{E}^* = \argmax_\mathcal{E} \left\{\mathbb{E}_{(s_i, s_j) \in \mathcal{S} \times \mathcal{S}} \left[ d(\mathcal{E}(s_i), \mathcal{E}(s_j)) \right] \right\} \\
     & \text{s.t.} \quad \forall (s_i, a_i), (s_j a_j) \in \mathca;{S} \times \mathcal{A} \\
     & \quad\quad\text{Smoothness Constraint: } \\
     & \quad\quad\quad d(z_{si}, z_{sj}) < 1 \land d(z_{ai}, z_{aj}) < 1 \iff d(z_{si}', z_{sj}') < 1 \\
     & \quad\quad\text{Radius Constraint: } \\
     & \quad\quad\quad d(z_{ai}, z_{aj}) < 2R
\end{align*}

\section{Loss Functions}

Now we will define a set of loss functions which optimize the dispersion objective and enforces the constraints.

\subsection*{Reconstruction Loss}
The first loss trains $\mathcal{E}$ and $\mathcal{D}$ to be a standard auto-encoder.

$$\mathcal{L}_\text{reconstruction} = -\ln[p(\mathcal{D}(\mathcal{E}(s)) = s)] - \ln[p(\mathcal{D}(\mathcal{E}(a, s), \mathcal{E}(s)) = a)]$$
$$\text{Where } s \text{ and } a \text{ are a state action pair sampled from the experience buffer.}$$

\subsection*{Forward Loss}
The next loss function trains the forwards model $\mathcal{F}$ to model the dynamics in the latent space.

$$\mathcal{L}_{\text{forward}} = -\ln[p(E(s_i+1) = FE(s_i, a_i))]$$
$$\text{Where } s_i \text{ and } a_i \text{ are a state action pair sampled from the experience buffer.}$$


Next we create a loss function which encourages the states and actions in latent space to be smooth with respect to the system dynamics.
% This means that small perturbations in the latent source state and action input to the forward model should result in proportionally small changes to the resultant predicted next state.
To realize this loss we define neighbor distributions in the state and actions space $\mathcal{H}_s$ and $\mathcal{H}_a$ such that $d(E(s) + \epsilon_s, E(s)) \leq 1 \text{, } e_s \sim \mathcal{H}_s$ and $d(E(a, s) + \epsilon_a, E(a, s)) \leq 1 \text{, } e_a \sim \mathcal{H}_a$ where


$$\mathcal{L}_\text{smooth} = d(F(E(s_i) + \epsilon_s, E(a_i, s_i) + \epsilon_a), E(s_{i+1}))$$
$$\epsilon_s \sim \mathcal{H}_s$$
$$z^s_j \sim p(z^s_i | \mu=z^s_\mu, \sigma^2=\mathbf{I})$$


There exists a set of state and action encoders that have the previously mentioned properties.
The easiest encoders to create in this set are the ones where all states and actions are within the neighborhood of all other states and actions.
The most interesting encoder is the one that differentiates states which behave similarly from ones that do not.
So the encoder that we are interested in is the one where the set of states that are in a neighborhood is the minimal one possible under our other constraints.
We can encourage our encoder to embody this property with this loss.

$$\mathcal{L}_\text{disperse} = -\frac{1}{N}\sum\limits_i^N \text{sigmoid}\log(d({z^s_i}', {z^s_\mu}'))$$
$${z^s_j}' \sim p({z^s_j}'|z^s_j,z^a_j)$$
$$z^s_j \sim p(z^s_j | \mu=z^s_\mu, \sigma^2=\mathbf{I})$$


There is one more problem, which corresponds to the degenerate solutions in which an entire neighborhood of actions all map to pretty much the same action, rather than to distinct but similar actions.
To avoid this degenerate possibility we will add a force pushing all of the actions together

$$\mathcal{L}_\text{condense} = \frac{1}{N}\sum\limits_i^N \text{sigmoid}\log(d({z^a_i}, {z^a_\mu}))$$
$$z^a_j \sim p(z^a_j | \mu=z^a_\mu, \sigma^2=\mathbf{I})$$

% Next we want to create a loss function which will enforce a temporal locality to our latent system.
% We would like to create a system in our latent space which exhibits the property that a single step can never carry us to a very significantly different state.
% In order to do this we will introduce an additional constraint which ensures that our states can change at most by a certain distance for every unit of elapsed time.
% This however allows for a degenerate solution in which all states are very close together and so no states are significantly different from any other states.
% To prevent this we will enforce that from every state we can always reach a state that is at this maximum distance away.
% We will execute this by rolling out a policy which maximizes the step-to-step distance during a trajectory, and then minimizing this loss over that max step dist trajectory.

% $$\mathcal{L}_\text{time} = -\sum\limits_i \text{sigmoid}(|d(z^s_{i+1},z^s_i) - \Delta t|)$$
% $$z^s_{i+1} \sim p_\theta(z^s_{i+1}|s_{i+1})$$

Embody the reality that sometimes some states simply have transitions that deviate a lot.\\
We need a way to easily differentiate those actions from other ones.\\
We could say that actions with a norm of less than 1 should not change the state much.\\
Is there anything we can say about large norm actions?\\

So ok here is our space:\\

- Small actions make small state changes.\\
- How does a small action decode if there is no small change action.\\
- Potentially make a conditioned decoder to make possible actions.




% Additionally we want gradient descent in the space of states and actions to achieve desired next states to be smooth.
% In order to encourage the space to realize this property we will push nearby states and actions towards having similar jacobians through the latent forward model.
% The way we will do this is by sampling \(N\) latent state action pairs in the neighborhood of a source latent state action \(z^{sa}_\mu\) and putting them into this list \(\mathbf{z}^{sa}_{0:N-1}\).
% We will now take this list and feed each element through the latent forward model to get the predicted next latent states \(\mathbf{z}^{s'}_{0:N-1}\).
% We will now take the jacobians of each latent state in \(\mathbf{z}^{s}_{0:N-1}\) w.r.t. the latent state actions and stack them into the tensor \(\mathbf{J}^{\frac{z^{s'}}{z^{sa}}}_{0:N-1}\)
% We will now make an operation which is analogous taking the standard deviation of the jacobian matrices in the stack.
% First we find the mean jacobian \(J^{\frac{{z^s}'}{z^{sa}}}_\mu = \frac{1}{N}\sum\limits_{i=0}^{N-1} \mathbf{J}^{\frac{{z^s}'}{z^{sa}}}_{i}\).
% Now we can find the standard deviation of the jacobian matrices by taking the norm of the difference between each jacobian matrix and the mean jacobian matrix.

% $$\mathcal{L_\text{gradients smooth}}= \sum\limits_i = \|J^{\frac{z^{s'}}{z^{sa}}}_\mu - J^{\frac{z^{s'}}{z^{sa}}}_i\|_F$$

% $$\mathcal{L_\text{state regularization}}=-\sum\limits_i\max(\|{z_i^s}'-z_i^s\|-\lambda_\text{state}, 0)^2$$
% $\mathcal{L}_\text{action regularization}=-\sum\limits_{i,j} \max(\|z'^s_{i,j}-z^s_i\| - \lambda_\text{act}, 0)^2, z_{i,j}'^s\sim p(z'^s_{i,j}|z^s_i,z^a_{i,j}),z^a_{i,j}\sim p(z^a_{i,j}|\mu=z^a_i, \sigma^2=\lambda_\text{act}^2)$

\section{Interesting Questions}

\begin{itemize}
    \item What happens if we use a very simple latent dynamics model? If this works then the encoder has to learn an encoding which knows about the dynamics of the system
    \item What is different about evaluating the loss function on the generated trajectory vs. on the predicted trajectory? Will the controller game the system by making a trajectory generator input that causes the control to misbehave but generates a high reward?
\end{itemize}

\section{Expirements}

\begin{itemize}
    \item Run simple trajectory optimization after training on various optimal policy rollouts in dm control problems.
    \item Try unsupervised learning generating rollouts with bootstrapped trajectory optimization in brax ant.
\end{itemize}

% \section{Other Thoughts}

% What does it mean to take a step of gradient descent in this latent space?
% Let's start by examining the simplest case with just one single state.

% \section{Network Losses}

% $\mathcal{L}_\text{action encoder}=\mathcal{L}_\text{forward} + \mathcal{L}_\text{smooth}+\mathcal{L}_\text{reconstruction}$

% Formalize more what you're trying to achieve here, what does SMOOTHNESS mean

% Describe what expirements you're running in the overleaf

\section{Controller}

\begin{definition}[Trajectory Generator]
    $f: \mathcal{X} \rightarrow \mathcal{S}^H$
\end{definition}
\begin{definition}[Latent Dynamics]
    $d: \mathcal{Z}^s \times \left(\mathcal{Z}^a\right)^H \rightarrow (\mathcal{Z}^s)^H$
\end{definition}
\begin{definition}[Cost Function]
    $j: (\mathcal{S} \times \mathcal{A})^H \rightarrow \mathbb{R}$
\end{definition}
\begin{definition}[Control Plan]
    A sequence $\mathbf{z^a} = \{z^a_0, z^a_1, ..., z^a_{H-1}\}$ where each $z^a_i \in \mathcal{Z}^a$ represents an action taken at time step $i$
\end{definition}
\begin{definition}[Control Cost]
    $c(\mathbf{z}_{0:H-1}^s, \mathbf{z}_{0:H-1}^{s*}) = \sum\limits_{i=0}^{H-1} \| z^s_i\ - z^{s*}_i \|_2^2$
\end{definition}

\begin{algorithm}[H]
    \caption{Latent Trajectory Optimization}
    \label{alg:lto}
    \begin{algorithmic}[1]
        \Require Initial state \(s_0\), initial guess for actions \(\mathbf{z}_{1:H-1}^a\), initial guess for input \(\mathcal{X}\), cost function \(c(\cdot)\), learning rate \(\eta\)
        \Ensure Optimal control sequence \(\mathbf{z}_{1:H-1}^{\text{opt}}\)

        \Function{LatentTrajectoryOptimization}{$s_0$, $\mathbf{z}_{1:H-1}^a$, $c(\cdot)$, $\mathcal{X}$, $\eta$}
        % Algorithm steps
        \Repeat
        \State Generate Target States: \(\mathbf{z}_{1:H}^{s*} = f(\mathcal{X})\)
        \State Do Latent Rollout: \(\mathbf{z}_{0:H-1}^s = d(s_0, \mathbf{z}_{0:H-1}^a)\)
        \State Evaluate Control Cost: \(c = c(\mathbf{z}_{0:H-1}^s, \mathbf{z}_{0:H-1}^{s*})\)
        \State Decode States and Actions: \(\mathbf{s}_{0:H-1} = g(\mathbf{z}_{0:H-1}^s), \mathbf{a}_{0:H-1} = g(\mathbf{z}_{0:H-1}^a)\)
        \State Evaluate Cost Function: \(j = j(\mathbf{z}^s_{0:H-1}, )\)
        \State Compute Gradients:
        \State \hspace{\algorithmicindent} Reached States w.r.t. Target States: \(\nabla_{{\mathbf{z}^s}^*}\mathbf{z}^s = \frac{\partial d}{\partial{\mathbf{z}^s}^*}\)
        \State \hspace{\algorithmicindent} Control Cost w.r.t. Reached States \(\nabla_{\mathbf{z}^s} c = \frac{\partial c}{\partial\mathbf{z}^s}\)
        \State \hspace{\algorithmicindent} Control Cost w.r.t. Actions \(\nabla_{\mathbf{z}^a} c = \frac{\partial c}{\partial \mathbf{z}^a}\)
        \State \hspace{\algorithmicindent} Cost Function w.r.t. Reached States \(\nabla_{\mathbf{z}^s} j = \frac{\partial j}{\partial \mathbf{z}^s}\)
        \State Propagate gradients through trajectory generator: \(\nabla_{\mathcal{X}} = (\nabla_{\mathbf{z}^s} c + \nabla_{\mathbf{z}^s}j ) \left( \nabla_{{\mathbf{z}^s}^*}{\mathbf{z}^s} \right) \left(\frac{\partial f}{\partial \mathcal{X}} \right)\)
        \State Adjust latent actions: \(\mathbf{z}_{1:H-1}^a = \mathbf{z}_{1:H-1}^a - \eta \nabla_{\mathbf{z}^a} c\)
        \State Adjust input to trajectory generator: \(\mathcal{X} = \mathcal{X} - \eta \nabla_{\mathcal{X}}\)
        \Until{Converged}

        \State \Return \(\mathbf{z}_{1:H-1}^{\text{opt}} = \mathbf{z}_{1:H-1}^a\)
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\end{document}
