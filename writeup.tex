\documentclass{article}
\usepackage{graphicx} % Required for inserting images\
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}

% Dark mode stuff, get rid of when sharing
\usepackage{xcolor}
\pagecolor[rgb]{0.15,0.15,0.15}   % Dark gray
\color[rgb]{0.9,0.9,0.9}          % Light gray

\newtheorem{definition}{Definition}

\title{Go By Vibes}
\author{Misha Lvovsky}
\date{August 2023}

\begin{document}

\maketitle

\section{Problem Statement}

We want to create a smooth embedding space which is analogous to our joint space.
Let's define that a neighborhood in our embedding space has a size of $1$ unit. This means that within $1$ unit all actions and states should be similar and should yield similar results. We can tie these vicinities together by saying that a certain vicinity should correspond with a certain amount of elapsed time. A certain action should be in the vicinity of another action if the resultant states they reach from a given source state are themselves in the same smaller vicinity.

\section{Spaces}

\begin{definition} [State Space] Let $\mathcal{S}$ signify the space of all possible states for the system.
\end{definition}
\begin{definition} [Action Space]
    Let $\mathcal{A}$ signify the space of all possible actions the agent can make
\end{definition}
\begin{definition} [Embedding Space]
Let $\mathcal{Z}^s$ and $\mathcal{Z}^a$ signify the spaces of all possible embeddings of states and actions respectively.
\end{definition}

\section{Loss Functions}

The first loss functions simply encourages the latent states and actions to be related to the actual states and actions.
The log avoids the gradient vanishing, and the sigmoid prevents it from exploding.
$$\mathcal{L}_\text{state reconstruction} = -\text{sigmoid}(\log(p_\theta(s|z^s))), z^s \sim p_\theta(z^s | s)$$
$$\mathcal{L}_\text{action reconstruction} = -\text{sigmoid}(\log(p_\theta(a|z^a))), z^a \sim p_\theta(z^a | a)$$
The next loss function trains a latent dynamics model to propogate the dynamics in the latent space.
Additionally when applied to the state and action encoders this loss function encourages the states and actions to encode enough information about the state and action to predict the next state.
$$\mathcal{L}_{\text{forward}} = D_{KL}\left( p_\psi\left({z^s}'\middle|z^s,z^a\right) \parallel p_\theta\left({z^s}'\middle| s\right) \right)$$ \\
The next loss function will be the defining property of our latent space which is that distances in our latent space should correspond to the amount of time it takes to get from one state to another.
There is a caveat to this which is that the time will be counted both in the forward and backward direction, so even if one state is unreachable from another state, their distance should still be well defined, and might even be low if they have some nearby common ancestor.
Ultimately this caveat will not deter us because it will still imbue the latent space with a particular measurement of similarity which is rooted in the dynamics of the robot.
$$\mathcal{L}_\text{time} = -\sum\limits_i \text{sigmoid}(|d({z^s}',z^s_i) - \Delta t|), {z^s}' \sim p_\theta\left({z^s}'\middle|z^s_i,z^a_i\right)$$
Next we create a loss function which encourages the states and actions in latent space to be smooth with respect to the transition function.
This means that small perturbations in the latent source state and action input to the forward model should result in proportionally small changes to the resultant precticted next state.
We do this by sampling $N$ states in the neighborhood of our mean state $z_\mu^s$ and propogating those states through the latent forward model to get the predicted next states.
All of the states should remain approximately within the neighborhood of the mean state's next state ${z_\mu^s}'$.
Like before the log and sigmoid are used to prevent the gradient from vanishing or exploding and the sum is divided to keep the total reward between $0$ and $1$.
$$\mathcal{L_\text{smooth}} = -\frac{1}{N}\sum\limits_j^N \text{sigmoid}(\log(p({z^s_j}'|\mu={z^s_\mu}',\sigma^2=\mathbf{I}))), {z^s_j}' \sim p({z^s_j}'|z^s_j,z^a_j)$$
Additionally we want gradient descent in the space of states and actions to achieve desired next states to be smooth.
In order to encourage the space to realize this property we will push nearby states and actions towards having similar jacobians through the latent forward model.
The way we will do this is by sampling \(N\) latent state action pairs in the neighborhood of a source latent state action \(z^{sa}_\mu\) and putting them into this list \(\mathbf{z}^{sa}_{0:N-1}\).
We will now take this list and feed each element through the latent forward model to get the predicted next latent states \(\mathbf{z}^{s'}_{0:N-1}\).
We will now take the jacobians of each latent state in \(\mathbf{z}^{s}_{0:N-1}\) w.r.t. the latent state actions and stack them into the tensors \(\mathbf{J}^{\frac{z^{s'}}{z^{sa}}}_{0:N-1}/\)
We will now make an operation which is analogous taking the standard deviation of the jacobian matrices in the stack.
First we find the mean jacobian \(J^{\frac{{z^s}'}{z^{sa}}}_\mu = \frac{1}{N}\sum\limits_{i=0}^{N-1} \mathbf{J}^{\frac{{z^s}'}{z^{sa}}}_{i}\).
Now we can find the standard deviation of the jacobian matrices by taking the norm of the difference between each jacobian matrix and the mean jacobian matrix.
$$\mathcal{L_\text{gradients smooth}}= \sum\limits_i = \|J^{\frac{z^{s'}}{z^{sa}}}_\mu - J^{\frac{z^{s'}}{z^{sa}}}_i\|_F$$

% $$\mathcal{L_\text{state regularization}}=-\sum\limits_i\max(\|{z_i^s}'-z_i^s\|-\lambda_\text{state}, 0)^2$$
% $\mathcal{L}_\text{action regularization}=-\sum\limits_{i,j} \max(\|z'^s_{i,j}-z^s_i\| - \lambda_\text{act}, 0)^2, z_{i,j}'^s\sim p(z'^s_{i,j}|z^s_i,z^a_{i,j}),z^a_{i,j}\sim p(z^a_{i,j}|\mu=z^a_i, \sigma^2=\lambda_\text{act}^2)$

\section{Interesting Questions}

\begin{itemize}
    \item What happens if we use a very simple latent dynamics model? If this works then the encoder has to learn an encoding which knows about the dynamics of the system
    \item What is different about evaluating the loss function on the generated trajectory vs. on the predicted trajectory? Will the controller game the system by making a trajectory generator input that causes the control to misbehave but generates a high reward?
\end{itemize}

\section{Network Losses}

$\mathcal{L}_\text{action encoder}=\mathcal{L}_\text{forward} + \mathcal{L}_\text{smooth}+\mathcal{L}_\text{reconstruction}$

% Formalize more what you're trying to achieve here, what does SMOOTHNESS mean

% Describe what expirements you're running in the overleaf

\section{Controller}

\begin{definition}[Trajectory Generator]
    $f: \mathcal{X} \rightarrow \mathcal{S}^H$
\end{definition}
\begin{definition}[Latent Dynamics]
    $d: \mathcal{Z}^s \times \left(\mathcal{Z}^a\right)^H \rightarrow (\mathcal{Z}^s)^H$
\end{definition}
\begin{definition}[Cost Function]
    $j: (\mathcal{S} \times \mathcal{A})^H \rightarrow \mathbb{R}$
\end{definition}
\begin{definition}[Control Plan]
    A sequence $\mathbf{z^a} = \{z^a_0, z^a_1, ..., z^a_{H-1}\}$ where each $z^a_i \in \mathcal{Z}^a$ represents an action taken at time step $i$
\end{definition}
\begin{definition}[Control Cost]
    $c(\mathbf{z}_{0:H-1}^s, \mathbf{z}_{0:H-1}^{s*}) = \sum\limits_{i=0}^{H-1} \| z^s_i\ - z^{s*}_i \|_2^2$
\end{definition}

\begin{algorithm}[H]
    \caption{Latent Trajectory Optimization}
    \label{alg:lto}
    \begin{algorithmic}[1]
        \Require Initial state \(s_0\), initial guess for actions \(\mathbf{z}_{1:H-1}^a\), initial guess for input \(\mathcal{X}\), cost function \(c(\cdot)\), learning rate \(\eta\)
        \Ensure Optimal control sequence \(\mathbf{z}_{1:H-1}^{\text{opt}}\)

        \Function{LatentTrajectoryOptimization}{$s_0$, $\mathbf{z}_{1:H-1}^a$, $c(\cdot)$, $\mathcal{X}$, $\eta$}
        % Algorithm steps
        \Repeat
        \State Generate Target States: \(\mathbf{z}_{1:H}^{s*} = f(\mathcal{X})\)
        \State Do Latent Rollout: \(\mathbf{z}_{0:H-1}^s = d(s_0, \mathbf{z}_{0:H-1}^a)\)
        \State Evaluate Control Cost: \(c = c(\mathbf{z}_{0:H-1}^s, \mathbf{z}_{0:H-1}^{s*})\)
        \State Decode States and Actions: \(\mathbf{s}_{0:H-1} = g(\mathbf{z}_{0:H-1}^s), \mathbf{a}_{0:H-1} = g(\mathbf{z}_{0:H-1}^a)\)
        \State Evaluate Cost Function: \(j = j(\mathbf{z}^s_{0:H-1}, )\)
        \State Compute Gradients:
        \State \hspace{\algorithmicindent} Reached States w.r.t. Target States: \(\nabla_{{\mathbf{z}^s}^*}\mathbf{z}^s = \frac{\partial d}{\partial{\mathbf{z}^s}^*}\)
        \State \hspace{\algorithmicindent} Control Cost w.r.t. Reached States \(\nabla_{\mathbf{z}^s} c = \frac{\partial c}{\partial\mathbf{z}^s}\)
        \State \hspace{\algorithmicindent} Control Cost w.r.t. Actions \(\nabla_{\mathbf{z}^a} c = \frac{\partial c}{\partial \mathbf{z}^a}\)
        \State \hspace{\algorithmicindent} Cost Function w.r.t. Reached States \(\nabla_{\mathbf{z}^s} j = \frac{\partial j}{\partial \mathbf{z}^s}\)
        \State Propagate gradients through trajectory generator: \(\nabla_{\mathcal{X}} = (\nabla_{\mathbf{z}^s} c + \nabla_{\mathbf{z}^s}j ) \left( \nabla_{{\mathbf{z}^s}^*}{\mathbf{z}^s} \right) \left(\frac{\partial f}{\partial \mathcal{X}} \right)\)
        \State Adjust latent actions: \(\mathbf{z}_{1:H-1}^a = \mathbf{z}_{1:H-1}^a - \eta \nabla_{\mathbf{z}^a} c\)
        \State Adjust input to trajectory generator: \(\mathcal{X} = \mathcal{X} - \eta \nabla_{\mathcal{X}}\)
        \Until{Converged}

        \State \Return \(\mathbf{z}_{1:H-1}^{\text{opt}} = \mathbf{z}_{1:H-1}^a\)
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\end{document}
